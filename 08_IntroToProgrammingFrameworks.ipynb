{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to programming frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning frameworks\n",
    "\n",
    "Choosing deep learning frameworks:\n",
    "- Ease of programming (development and deployment)\n",
    "- Running speed\n",
    "- Truly open\n",
    "    - The bad thing about some open source frameworks is that they do not have good goverance. So while it could be open source, it might suffer from one single goverance closing the framework\n",
    "\n",
    "\n",
    "Deep Learning frameworks:\n",
    "- Caffe/Caffe1\n",
    "- CNTK\n",
    "- DL4J\n",
    "- Lasagne\n",
    "- mxnet\n",
    "- PaddlePaddle\n",
    "- TensorFlow\n",
    "- Theano\n",
    "- Torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_1:0' shape=() dtype=float32_ref>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Add_3:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cost Function: w^2 - 10w +25\n",
    "cost = tf.add(tf.add(w**2, tf.multiply(-10., w)), 25)\n",
    "\n",
    "# For better sytnax, you could use the following\n",
    "# w = w**2 - 10*w + 25\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'GradientDescent' type=NoOp>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params are learning rate\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Only after running tf.global_variables_initializer() in a session will your variables \n",
    "# hold the values you told them to hold when you declare them\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# A session allows to execute graphs or part of graphs. It allocates resources \n",
    "# (on one or more machines) for that and holds the actual values of intermediate results and variables\n",
    "session = tf.Session()\n",
    "\n",
    "session.run(init)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- At this point, we are only looking at the function. We have no updated the values with any learning parameters. Thus, the result of 0. Follow step 2 and youll see that we update the value close to 5. Remember, we are trying to minimize the cost function. With our equation, we know that we need 5 as w to get a cost function of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099999994\n"
     ]
    }
   ],
   "source": [
    "session.run(train)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "- We will now do a for loop to see how the values will get close to 5\n",
    "- Also, since we are optimizing for the variable w, we make the w a variable within the tensorflow enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9999886\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session.run(train)\n",
    "    \n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What if the optimization occurs for the training data? This is not as straight forward as the example above, because the training data will continously change...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.],\n",
       "       [-10.],\n",
       "       [ 25.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1.], [-10.], [25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "coefficients = np.array([[1.], [-10.], [25]])\n",
    "\n",
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32, [3,1])\n",
    "\n",
    "# Notice that the parameters of above were 1, -10, and 25\n",
    "# tf.placeholder: you are telling tensorflow that youll be providing information on the parameters\n",
    "cost = x[0][0]*w**2 + x[1][0]*w - x[2][0]\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "print(session.run(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099999994\n"
     ]
    }
   ],
   "source": [
    "session.run(train, feed_dict={x: coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9999886\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session.run(train, feed_dict={x: coefficients})\n",
    "    \n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- The results are the same as before... which indicates everything went smoothly\n",
    "- A placeholder is a variable whose value you assign later\n",
    "- This is converniate way to get your training data into the cost function\n",
    "- This is helpful when we begin to add mini-batches into the data\n",
    "- A standard TensforFlow code is:\n",
    "    ```python\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "print(session.run(w))```\n",
    "- which coule be replaced by \n",
    "```python\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    print(session.run(w))```\n",
    "    \n",
    "- Another thing is through out equation, the dictionary and the variable, we are computing the forward propogation. \n",
    "- Since we are computing the forward propogation through TensorFlow, we can compute the backward propogation with the build-in tools from TensorFlow\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
