{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning process\n",
    "\n",
    "Importance of hyperparamets:\n",
    "\n",
    "- First:\n",
    "    - learning rate (alpha) is the most important hyperparameter!\n",
    "\n",
    "- Second:\n",
    "    - beta: .9\n",
    "    - num of hidden units\n",
    "    - mini-batch size\n",
    "\n",
    "- Third:\n",
    "    - num of layers\n",
    "    - learning rate decay\n",
    "    \n",
    "- Fourth:\n",
    "    - beta1: 0.9\n",
    "    - beta2: 0.999\n",
    "    - eplison: $10^{-8}$\n",
    "\n",
    "\n",
    "Historically, practioniers would create a grid and try all the possibilities. This was feasible approach because there were not many hyperparameters to choose from. Thus, we would have to try 25 values if we were to have 5 possible value for 2 hyperparameters\n",
    "\n",
    "But what is a better method? A better method would be to choose randomly. This is a better method because we are choosing from a wider pool of hyperparameters. When we follow a structure of 5 hyperparmets (say for learning rate and epilson), we are have to choose our hyperparameters from 5 learning rate. Even though we have a grid that has 25 possibilities, we are using the learning rate 5 times rather than 25.\n",
    "\n",
    "However, if the values are choosen randomly, we are confined to actually choosing from 5 learning rate but 25 distinct learning rates.\n",
    "\n",
    "<img src=\"./images/improv_53.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Another method: **Coarse to fine**\n",
    "- We would find hyperparamters that have the potential to be the optimal and then make our subset zoom into a region within the most optimal hyper parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an appropriate scale to pick hyperparameters\n",
    "\n",
    "The takeway is to make sure you are choosing your hyperparameters from the correct distribution. For some hyperpameters, choosing from a uniform distribution make sense.\n",
    "- Example: total number of layers, or number of hidden units.\n",
    "\n",
    "\n",
    "However, this is not always the case. For other hyperpameters, a uniform distribution does not make sense.\n",
    "\n",
    "In the image below, if we are sampling from a normal distribution between 0.0001 and 1, most of the values would NOT be in between 0.0001 and 0.01... even though it should be\n",
    "- <img src=\"./images/improv_36.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "- It's more reasonable to search for a learning rate across a log scale!\n",
    "- <img src=\"./images/improv_54.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "```python\n",
    "r = -4 * np.random.rand() # r is in btw [-4, 0]\n",
    "learning_rate = np.power(10, r) # hence, 10^r would be 10^-4 through 10^0\n",
    "```\n",
    "\n",
    "Hyperparamters for exponentially weighted averages:\n",
    "- Beta could be 0.9 (10 days) or 0.999 (1000 days) \n",
    "- Another implication where sampling from a logistic distribution makes sense is when you are calculating for the betas in our weighted average... notice that we are finding the beta between the value 0.9 and 0.999\n",
    "    - BUT we are using 1-beta. Hence, we are finding the learning rate between 0.1 and .001\n",
    "\n",
    "<img src=\"./images/improv_37.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "It's bad to sample from a normal distribution for some hyperparameters because the changes are sensitive when the values are close to 1\n",
    "- Meaning, when the changes are closer to 1, the betas will have a greater change \n",
    "- Hence, using a log distribution allows for our model to sample from learning rates that are close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning in practice: Pandas vs. Caviar\n",
    "\n",
    "There are two tuning methods:\n",
    "1. Babysitting one model: Here, we would use certain hyperparameters, check it performance and tweak it.\n",
    "    - <img src=\"./images/improv_38.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "2. Train many models in parallel!\n",
    "    - <img src=\"./images/improv_39.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "Reminder: \n",
    "- Pandas: Train one model and try to change it manually. \n",
    "    - Pandas typically have one kid and invest all their resources to on kid\n",
    "- Cavier: Change many models in parallel and let it run by itself with different set of hyperparameters.\n",
    "    - If we have a lot of computational power, the caviar could be a great method to use, we are training it in parallel!\n",
    "    - Cavier typically have many kids and hope that one kid will turn out well!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
