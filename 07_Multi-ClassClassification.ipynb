{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "- Softmax regression are used when we have to classified across multiple classes (not just yes and no)\n",
    "- The output probability will add to 1\n",
    "\n",
    "<img src=\"./images/improv_47.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- Notice that we are using the e function to calculate the values for the output. This helps make make small changes between the values into larger changes when we apply the $e$ function. \n",
    "- After performing the calculation, we add all the values and then divide our individual observation against each value (normalizing the values between 0 and 1)\n",
    "\n",
    "<img src=\"./images/improv_48.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "- The image above makes sense but why are using the $e$ function?\n",
    "    - The changes between the values using the $e$ function is able to differential small changes into larger changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a softmax classifier\n",
    "\n",
    "We call the type of calculation softmax. This is the opposite of hardmax, which would have assign binary values. (1 to the most predictive response, while 0's the remaining possible outcomes) \n",
    "\n",
    "Softmax regression gernalizes logistic regression to C classes\n",
    "- If C=2, softmax reduces to logistic regression!\n",
    "\n",
    "<img src=\"./images/improv_49.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "\n",
    "- We need to make the right function, $-logyhat_2$ small and the only way to do is to make the $yhat_2$ very large. This minimizes the loss function. Remember that when we have a negative log function, we would have a large y hat value indicating that we were highly sure of our prediction.\n",
    "\n",
    "**Set up our ground/predictions**\n",
    "\n",
    "<img src=\"./images/improv_56.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "\n",
    "**Hence, when we derive the derivatives, we would have to find the derivatives off all the possibles outcomes.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
