{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing activations in a network\n",
    "\n",
    "We have seen how normalizing can help us train faster. We cannot expect inputs to all fit within the same range. \n",
    "\n",
    "We are able to normalize the inputs values but what about normalizing the values in the hidden layers?\n",
    "- *Sidenote*\n",
    "    - There's often debate of what whether we should normalize after or before the activation function\n",
    "    - Most people do it after the activation functions\n",
    "    \n",
    "The image below describes normalizing the z in a specific hidden layer\n",
    "- Like shown in the previous section, we must calculate the mean and variance to find normalize the values\n",
    "- One thing to note is that we might not always want our values to be normalzied around 0 with a standard deviation of 1.\n",
    "    - Hence we have to additional parameters that we could tweak that could normalize the values across a different distribution\n",
    "        - $\\gamma$ (gamma) and $\\beta$ (beta)\n",
    "        - These values that we could modify...\n",
    "        - If gamma were to be $\\sqrt{\\sigma + \\epsilon}$, and beta to $\\mu$, we would inverse the the normalization\n",
    "        \n",
    "<img src=\"./images/improv_40.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Batch Norm into a neural network\n",
    "<img src=\"./images/improv_41.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "- Notice how we are finding the normalized value after each iteration. We are not finding the value after the activation. The new values are used as z for the activation. \n",
    "- We also have many new parameters that we have to calculate.\n",
    "\n",
    "\n",
    "This is typically done through mini-batches.\n",
    "<img src=\"./images/improv_42.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "\n",
    "Parameters: \n",
    "- We will not be using the beta (beta in the equation) since it is a constant that is applied to every example.\n",
    "- Thus, removing it, will not do anything since we will remove it from all the examples\n",
    "- The other beta term (found in the equation where we modified the normalization of z). It controls the way we shift (or the bias) in our problem (since it controls the mean of the normalization of z)\n",
    "\n",
    "<img src=\"./images/improv_43.png\" alt=\"Drawing\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does batch Norm work?\n",
    "- It makes weight of specific layer more robust to changes\n",
    "- If we are training a shallow network, or a deep network:\n",
    "    - If we only trained to black cats, or model will not do great against color cats\n",
    "    - This is called covariate shift: this means that if the mapping to x to y changes, we would have to retrain the model.\n",
    "    \n",
    "    \n",
    "In practice, we will face issues with our data. Since we will be using previous networks, we will need to find a way to capture how we map it. If we do not this, we will put in position where our model will suffer if the values from its previous network were to change.\n",
    "\n",
    "Since all the values in a layer will be somehow used for futures layers we wil need to find to normalized these values.\n",
    "\n",
    "This makes the job of the later layers easier because it will have similar values as input. These inputs values will not shift as much.\n",
    "\n",
    "<img src=\"./images/improv_44.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "\n",
    "With the normalization, it pushes the later layers not to rely on any one hidden unit, and thus normalization has a slight regularization effect\n",
    "\n",
    "<img src=\"./images/improv_45.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "\n",
    "In the image above, we notice that when predicting new vlaues for the neural network could lead to wrong answers if the values do not match to how the model were trained. One way to reduce is to make sure we have values with the same distribution (mean and variance)\n",
    "- So for example, their values could change but we would maintain the values to have a mean of 0 awith a variance of 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm at test time\n",
    "One of the drawbacks with the mini-batch is that we cannot test our testing set through mini-batches since we would typically have one example. And having the mean and variance does not make much sense.\n",
    "\n",
    "During test time, we would need to calculate a separate mean and variance\n",
    "- To calculate, we would estimate the values using an expontentially weighted average across mini-batches\n",
    "- We must store the $\\mu$ and $\\sigma$ for each of the mini-batches and use those values to calcualte the exponentially weighed averages\n",
    "\n",
    "<img src=\"./images/improv_46.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
