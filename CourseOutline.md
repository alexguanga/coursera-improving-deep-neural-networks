# Improving Deep Neural Networks

1. Setting up your Machine Learning Application
  - Train/Dev/Test Sets
  - Bias/Variance
  - Basic Recipe for Machine Learning
2. Regularizing Your Neural Network
  - Regularization
  - Why Regularization Reduces Overfitting?
  - Dropout Regularization
  - Understanding Dropout
  - Other Regularization Methods
3. Setting Up Your Optimization Problem
  - Normalizing Inputs
  - Vanishing/Exploding Gradients
  - Weights Initialization for Deep Networks
  - Numerical Approximation of Gradients
  - Gradient Checking
  - Gradient Checking Implementation Notes
4. Optimizing Algorithms
  - Mini-batch Gradient Descent
  - Understanding Mini-batch Gradient Descent
  - Exponentially Weighted Averages
  - Understanding Exponentially Weighted Averages
  - Bias Correction in Exponentially Weighted Averages
  - Gradient Descent with Momentum
  - RMSprop
  - ADAM Optimization Algorithm
  - Learning Rate Decay
  - The Problem of Local Optima
5. Hyperparameter Tuning
  - Tuning Process
  - Using an Appropriate Scale to Pick Hyperparameters
  - Hyperparameters Tuning in Practice: Pandas vs. Caviar
6. Batch Normalization
  - Normalizing Activations in Network
  - Fitting Batch Norm Into a Neuron Network
  - Why does Batch Norm Work?
  - Batch Norm at Test Time
7. Multi-Class Classification
  - Softmax Regression
  - Training a Softmax Classifier
