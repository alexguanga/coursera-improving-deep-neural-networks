{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizing your neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "<img src=\"./images/improv_3.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "- Notice that we are adding a proportional regularization to W!\n",
    "- The equation above is the L2 regularization\n",
    "- We do not to add the b regularization. In practice, you could. W is a high-dimensional parameter! B is typically a single number.\n",
    "\n",
    "<img src=\"./images/improv_4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- While this is the same as the L2 norm, it is not called L2 norm. Instead, it is called the `Frebenius Norm`\n",
    "\n",
    "\n",
    "Implementing the regularization (affects backprop)\n",
    "<img src=\"./images/improv_5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Substuting the formulas:\n",
    "- Also noticed, that is sometimes called the weight decay since we are mult. the weight by a fraction (which is than 1) in addition to the gradient of W\n",
    "\n",
    "<img src=\"./images/improv_6.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We add the regularization term because we do want to prevent the cost function from decreasing. Hence, we add the term. Also, when we compute the derivatives, we see that adding a term will further decrease the weights (a lower coefficient is a positive when you to prevent overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why regularization reduces overfitting?\n",
    "Remember, if we have a regularization that is large, than we would have the weight close to 0. Meaning, this model will not be able to overfit the data.\n",
    "\n",
    "Remember, as you increase the regularization, we are reducing the gradient by a lot.\n",
    "\n",
    "Another way of understanding this:\n",
    "- When we have a large lambda, we will have smaller weights.\n",
    "- With the smaller weights, we will have a smaller activation function. If we look at the tanh function, we will have our activation around the linear portion of the graph not the outier portion.\n",
    "- It will be every layer is like a linear function. Thus, we will not be able to compute complicated models but only simple linear models across the network.\n",
    "- <img src=\"./images/improv_7.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization\n",
    "In the image below, each node has a 50 percent probability of being removed.\n",
    "- Once the node is removed, we will remove any incoming/outgoing links to that node. \n",
    "\n",
    "- <img src=\"./images/improv_8.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "\n",
    "\n",
    "How is dropout regularization done?\n",
    "- We have to use \"inverted dropout\"\n",
    "```python\n",
    "# If we are using the dropout at layer 3\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep.prob\n",
    "a3 = np.multiple(a3, d3)\n",
    "a3 /= keep.prob\n",
    "```\n",
    "- If the keep.prob is 0.8, then there is .2 prob that a neuron will be dropped off.\n",
    "- The last step is a bit weird. We have to invert back the expected value even after we dropout neurons.\n",
    "- Think of it like this. We are training with fewer neurons. Yet, we will need to use all the neurons when we are training. Thus, the expected value must remain the same (to train the network more efficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Dropout\n",
    "Intuition: Cannot rely on any one feature, so have to spread out weights since any one feature could be dropped out!\n",
    "\n",
    "You can also have a keepprops for different layer.\n",
    "- <img src=\"./images/improv_9.png\" alt=\"Drawing\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other regularization methods\n",
    "\n",
    "Data augmentation:\n",
    "- You can train with more examples by changing your format (like images)\n",
    "- Think of it like adding noise, rotating, or changing it the position.\n",
    "- We can get more data this way without having to get more training example\n",
    "\n",
    "\n",
    "Early stopping:\n",
    "- <img src=\"./images/improv_10.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "- Noticed that if we stop mid way, we will not have large weight, thus the network will not have learned the data to well. \n",
    "- Since larger are weights will make the network perform better for the training set than the dev set\n",
    "- Drawback\n",
    "    - Orthongonaliation: Thus, meaning we are trying to do two things at once, where we are not doing a good job at optimizing and we are trying to not overfit!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
